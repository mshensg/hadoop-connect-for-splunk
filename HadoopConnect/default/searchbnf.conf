[runexport-command]
syntax      = runexport name=<string> (forcerun=<bool>)? (roll_size=<number>) (maxspan=<number>)? (minspan=<number>)? (starttime=<epoc-time>)? (endtime=<epoc-time>)? (parallel_searches=<int>|max)? (kerberos_principal=<string>)? (format=raw|json|xml|csv)? (fields=<string list>)?
shortdesc   = Executes an HDFS export job specified by name. 
description = Executes an HDFS export job specified by name. This command is only meant to be ran by the scheduler - to override set forcerun=1. \i\\
              "forcerun"           - set to 1 if you want to bypass scheduler checking, default is not set. \i\\
              "roll_size"          - minimum file size at which point no more events are written to the file and \i\\
              				         it becomes a candidate for HDFS transfer, unit is MB, default 63MB. \i\\
              "maxspan"            - maximum number of index time seconds to run an export job for, unit is second, default 10 days. \i\\
              "minspan"            - minimum number of index time seconds to run an export job for, unit is second, default 30 minutes. \i\\
              "starttime"          - if there is no available cursor, what is the minimum index time to use, unix time.  \i\\
              "endtime"            - the maximum index time for which to export data , unix time. \i\\
              "format"             - output data format, supported values are raw | csv | tsv |  json | xml \i\\
              "fields"             - list of splunk event fields exported to export data, invalid fields will be ignored \i\\
              "parallel_searches"  - the number of parallel searches to spawn, each search targets a subset of indexers \i\\
              "kerberos_principal" - kerberos principal required for access HDFS destination path is hadoop is managed by Kerberos \i\\
              
comment1 = run export name MyData with roll size of 128MB
example1 = | runexport name=MyData forcerun=1 roll_size=128
category = exporting


[exporthdfs-command]
syntax      = exporthdfs basefilename=<string> dst=<string> (tmp_dir=<string>)? (rollsize=<number>) (maxlocal=<number>)? (exportname=<string>)? (compress=<number>)? (format=<string>)? (fields=<comma delimited string>)? 
shortdesc   = Executes an HDFS export for a given search query. 
description = Executes an HDFS export for a given search query. This command runs a specified search query and oneshot export search result to HDFS. It recognizes  \
              a special field in the input events, _dstpath, which if set will be used as a path to be appended to dst to compute final destination path. \i\\
              "basefilename"       - prefix of the export filename. \i\\
              "dst" 		       - destination base path where export data is stored, support both file:/// and hdfs:// schemes. \i\\
              "tmp_dir"		       - the absolute path of local temporary directory to keep temporary files. \i\\
              "rollsize"   	       - minimum file size at which point no more events are written to the file and \i\\
                                     it becomes a candidate for HDFS transfer, unit is "MB", default "64MB". \i\\
              "maxlocal" 	       - maximum allowable local disk usage at which point in-memory data and local files are \i\\
                                     flushing and exporting to HDFS, unit is "MB", default "1GB". \i\\
              "exportname"	       - the name of the export process. \i\\
              "compress"	       - gzip compression level from 0 to 9, 0 means no compression, higher number \i\\
 	                                 means more compression and slower writing speed, default 2. \i\\
              "format"             - output data format, supported values are raw | csv | tsv | json | xml \i\\
              "fields"             - list of splunk event fields exported to export data, invalid fields will be ignored \i\\
              "kerberos_principal" - kerberos principal required for access HDFS destination path is hadoop is managed by Kerberos \i\\

comment1 = Export all events from index "bigdata" to HDFS location "/myexport/&#060;YYYYmmdd&#062;/&#060;HH&#062;/&#060;host&#062;" on cluster "mynamenode" with "MyExport" as the prefix of export filenames. \
           Partitioning of the export data is achieved by eval preceeding the exporthdfs command.
example1 = index=bigdata | eval _dstpath=strftime(_time, "%Y%m%d/%H") + "/" + host | exporthdfs basefilename=MyExport dst=hdfs://mynamenode:9000/myexport/
comment2 = Export all events from index "bigdata" to HDFS location "/myexport/&#060;host&#062;/&#060;source&#062;/" on cluster "mynamenode" with "MyExport" as the prefix of export filenames
example2 = index=bigdata | exporthdfs basefilename=MyExport dst=hdfs://mynamenode:9000/myexport/
category = exporting

[hdfs-command]
syntax      = hdfs <directive> (<hdfs-location>)+ (<directive-options>)?
shortdesc   = Execute an HDFS directive and return search results.
description = Execute an HDFS directive and return search results. The following directives are currently supported directives: \i\\
              "read" - read the contents of the given files. Options: delim, fields \i\\
              "ls"   - list the contents of the given HDFS locations and bring back each entry as a search result \i\\
              "lsr"  - recursively list the contents of the given HDFS locations and bring back each entry as a search result \i\\
              Directive options: \i\\
              "delim"  - applicable to: read. Delimiter character to split the read lines into fields \i\\
              "fields" - applicable to: read. Comma delimited list of field names to assign to the segments created by the delimiter splitting \i\\ 
 
comment1 = Read MapReduce tab delimited result outputi, containing the following fields: city, lat, long, count
example1 = | hdfs read "hdfs://mynamenode:9000/path/to/mr/results" delim="\t" fields="city,lat,long,count"
comment2 = List all files and directories in the given paths
example2 = | hdfs ls "hdfs://mynamenode:9000/some/path" "hdfs://mynamenode:9000/some/other/path"
comment3 = Recursively list all files and directories in the given paths
example3 = | hdfs lsr "hdfs://mynamenode:9000/some/path" "hdfs://mynamenode:9000/some/other/path"

[movehdfs-command]
syntax      = movehdfs dst=<string>
shortdesc   = Executes an HDFS move to move local files to Hadoop storage
description = Executes an HDFS move to move local files to Hadoop storage. This command reads a list of file paths from input stream and move the files to Hadoop storage. 
              "dst"                - destination base path where export data is stored, support both file:/// and hdfs:// schemes. \i\\
              "tmp_dir"		       - the absolute path of local temporary directory to keep temporary files. \i\\
        
comment1 = Move the list of files specified in input stream to HDFS location "/myexport/&#060;YYYYmmdd&#062;/&#060;HH&#062;/&#060;host&#062;" on cluster "mynamenode" with "MyExport" as the prefix of export filenames. 
example1 = movehdfs dst=hdfs://mynamenode:9000/myexport/
category = exporting

